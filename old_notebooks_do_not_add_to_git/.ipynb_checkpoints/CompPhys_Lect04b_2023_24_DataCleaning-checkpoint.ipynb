{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMCmpZbhDuBH"
   },
   "source": [
    "# Computational Physics U24568\n",
    "## Lecture 4b - Reading and Cleaning Data with Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JryneyWXDuBJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZzPA_lBDDuBK"
   },
   "source": [
    "Today we'll be using a 2022 dataset from the Department for Environment, Food & Rural Affairs, downloaded from [this link](https://environment.data.gov.uk/water-quality/view/download/new).\n",
    "This dataset is available under the [Open Government Licence v3.0](https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/).\n",
    "\n",
    "The dataset lists all the water quality samples that DEFRA took in the Solent region in 2022, with information about the location, the type of water sampled, and what chemical measurements were made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6WEqfzBLDuBK"
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# Creates a DataFrame\n",
    "data = pd.read_csv('SSD-2022.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvQWtOssDuBK"
   },
   "source": [
    "CSVs are comma-separated variable files, meaning the data is just written one row a line with a comma between each column. It's a very common data format because it's so simple, but it's not very efficient. Often you'll find data instead in an Excel spreadsheet, and you can use Pandas `read_excel` to read those.\n",
    "\n",
    "The [Pandas User Guide](https://pandas.pydata.org/docs/user_guide/index.html) has information about all the functionality of Pandas, plus some getting started guides. In this notebook, we'll have just a short overview of some things that it can do. Chapter 3 of [Data Science Handbook](https://jakevdp.github.io/PythonDataScienceHandbook/) is a bit out of date but should also give some insight.\n",
    "\n",
    "To get a quick look at the data, we can use `data.head()` to see the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FEOBwT0wDuBL"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XkQrl2ryDuBL"
   },
   "source": [
    "Then `data.info()` will give us a summary of what type of data is in each column, and how many of these are valid values rather than missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqJZSBjXDuBL"
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFBiVXcxDuBL"
   },
   "source": [
    "To select a specific column like `@id`, do `data['@id']`, and you'll get a Pandas Series. You can treat that like a Numpy array. You can do a comparison or other function to get an array of Booleans (i.e. True, False), and then use it to slice the DataFrame (select just the rows where the result is True)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EYQLorW8DuBL"
   },
   "source": [
    "**Exercise**: Select the `sample.sampledMaterialType.label` column, and make an array of Booleans called `gndwtr` that indicate whether the label is `'GROUNDWATER'`. Use `np.sum` to count how many `True` values there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ev1QZX3RDuBL"
   },
   "outputs": [],
   "source": [
    "gndwtr ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DZu2pG_mDuBL"
   },
   "outputs": [],
   "source": [
    "np.sum(gndwtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HEdDAyWFDuBM"
   },
   "outputs": [],
   "source": [
    "# To get a new DataFrame with just these rows\n",
    "df_gndwtr = data[gndwtr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_auLd5sZDuBM"
   },
   "source": [
    "There is a function `.duplicated()` that returns True if the entry is a duplicate of one above. You can follow it with `.any()` if you want to check whether the column has any duplicates. You can also use `.drop_duplicates()` if you just want to keep the first of each uinque value.\n",
    "\n",
    "**Exercise**: The below code iterates through the names of columns in the dataset. Modify it to find which columns have duplicated values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HZfkGDGGDuBM"
   },
   "outputs": [],
   "source": [
    "for col in data.keys():\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "It2YX_u-DuBM"
   },
   "outputs": [],
   "source": [
    "for col in data.keys():\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKyM1tgYDuBN"
   },
   "source": [
    "**Exercise**: Very often datasets contain dates, which are encoded as strings. Use the `pd.to_datetime` to convert the dates in this dataset to a more useful object, then experiment or read the documentation to find how to get the time difference between two dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "px9cEu_CDuBN"
   },
   "outputs": [],
   "source": [
    "dates ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_HctjugKDuBN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_HsPIghDuBN"
   },
   "source": [
    "**Exercise**: Remove duplicate dates, then find and plot the difference in time between each unique date. You may need to sort these to get a good plot.\n",
    "\n",
    "There are many ways to do this, but if you want to apply a function to a Pandas Series, use the `.apply(func)` method, where `func` is a function, perhaps defined with a `lambda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hOfkMwmXDuBN"
   },
   "outputs": [],
   "source": [
    "unique_dates ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-5a_3n9RDuBN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KF046F9HDuBN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fy8PuUwNDuBN"
   },
   "source": [
    "Now we'll see another very useful function, `.groupby(column)`. It will group all entries by their values in one of the columns. So you can find all samples taken in the same location.\n",
    "\n",
    "This can be combined with `.count()` to see how many entries have each value. Or you can use `.groups` to get a dictionary of the different groups, which is useful for counting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jz9QLxx7DuBN"
   },
   "source": [
    "**Exercise**: Find out how many of each type of material were sampled (the `sample.sampledMaterialType.label` column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Ipej-jdDuBO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ba-LEFRDuBO"
   },
   "source": [
    "**Exercise**: Now select both the `sample.samplePoint.easting` and `sample.samplePoint.northing` columns (slice the DataFrame with a list of names). Drop the duplicates. How many different locations are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ivBs3cJKDuBO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrpetqNeDuBO"
   },
   "source": [
    "**Exercise**: Now convert these to a Numpy array and make a scatter plot to visualise where the samples are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lSyuF_Q3DuBO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dwnbmVIdDuBO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Glc9yGpxDuBP"
   },
   "source": [
    "**Exercise**: There are some outliers in the positions. How many times does this happen? What location(s) does it happen for? Why do you think this happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XvEmsfIKDuBP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9sLK5XDbDuBP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omWLhiX_DuBP"
   },
   "source": [
    "**Challenge**: Explore the dataset. How many types of measurements are made? How many are for compliance reasons? How often is a typical location sampled?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CBXux4MODuBQ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
