{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10b: Significance\n",
    "\n",
    "In 10a we looked at using repeated draws of a probability distibruion to find the value of somehting and to quantify uncertainty. \n",
    "\n",
    "Here we will use those same techniques to find the significance of a signal when it is burried in noisy data.\n",
    "\n",
    "Previously, we've cross-correlated data with a known signal shape. We observe spikes in the cross-correlation function, which seem to indicate the presence of a signal in the noisy data. But how can we be sure that this wasn't just some particularly unlucky noise? In reality, we never can be. We can only ask: how unlikely would it have been to see something like this in noise alone.\n",
    "\n",
    "So let's try to quantify that. We'll create a dataset with our `signal_2` added to it, but the noise will be louder than the data we used previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some settings for the data\n",
    "sample_rate = 32 # 32 samples per second\n",
    "num_data_samples = 128*sample_rate # 128 seconds worth of data\n",
    "times = np.arange(num_data_samples) / sample_rate\n",
    "\n",
    "def make_signal(gaussian_width, chirpiness):\n",
    "    signal_inst_frequency = 2. + chirpiness*np.sin(2 * np.pi * 0.1 * times)\n",
    "    phases = [0]\n",
    "    for i in range(1,len(times)):\n",
    "        phases.append(phases[-1] + 2 * np.pi * signal_inst_frequency[i] * 1./sample_rate)\n",
    "    signal = np.sin(phases)\n",
    "    gaussian = np.exp( - (times - 64)**2 / (2 * gaussian_width))\n",
    "    signal = gaussian * np.sin(phases)\n",
    "    return signal[48*sample_rate:80*sample_rate]\n",
    "\n",
    "# Make the signals\n",
    "signal_1 = make_signal(10., 0.)\n",
    "signal_2 = make_signal(1., 1.)\n",
    "signal_3 = make_signal(8., 4.)\n",
    "signal_4 = make_signal(10., 1.)\n",
    "\n",
    "# Make the noise, and add a signal to the noise at an unknown spot\n",
    "# Set seed so we get the same dataset!\n",
    "np.random.seed(21)\n",
    "noise = np.random.normal(size=[num_data_samples])\n",
    "rndi = np.random.randint(0,sample_rate*96)\n",
    "data_21 = noise\n",
    "data_21[rndi:rndi+len(signal_2)] += signal_2*0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "* Compute the cross-correlation of data_21 with signal_2. Can you clearly see the signal in the noise?\n",
    "* Create a 128-second long stretch of random noise (in the same way as we did for `data_21`, but now without a signal added). Cross-correlate with signal_2. What is the loudest value that you see?\n",
    "* Repeat the process 1000 times. How many times is the maximum cross-correlation louder than it was for `data_21`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The peak of the signal is just barely above the noise. You can change the $0.9$ in the code that generates the data to make the signal a bit stronger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will not get exactly the same numbers, but in my tests I saw 108 examples of noise where the cross-correlation with `signal_2` produced a larger value than for `data_21`. We know that each of those times, there actually was no signal in the data - because we generated the noise ourselves, and we know that we did not add a signal.\n",
    "\n",
    "So in 108 out of 1000 cases, we got a peak cross-correlation larger than what we had in `data_21`, but no signal was actually present. This means is that if we had received `data_21` from a detector, we would not be totally confident that there was a real signal in the data. We can however make a statistical statement of how confident we are.\n",
    "\n",
    "We would see a cross-correlation peak equal to or louder than the one in `data_21` in 72 out of 1000 times where there is no signal. We can express the signficance as a 10.8% chance that the signal is just a \"false alarm\". We can also make this a rate. Each segment of data is 128 seconds long. There are 108 false alarms in 128000 seconds of data, which is one per 1200 seconds, or about 3 per hour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making things faster\n",
    "\n",
    "It may take a minute or two to calculate 1000 cross-correlations. You may want to spend a few minutes to find out if either numpy or scipy has a correlation function that does the same thing as the cross-correlation code we've written. \n",
    "\n",
    "If you can find a matching function, you should test the output to make sure it does the same thing as the code we've written. Make sure you understand what the inputs and outputs of the function are.\n",
    "\n",
    "Numpy and scipy functions often much faster than code we write. You can run an example with `%timeit` placed in front to see how fast it runs. If you switch to using this for the following cross-correlation calculations, things will go much quicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The `scipy` function is fastest, but it gives a different-length result. This is because of the way it handles the edges, where the template runs off the ends of the data. See the plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's an optional argument `mode`. The default option of `mode='full'` gives the above behaviour. If we instead use `mode='valid'`, we get something that's the same as our convention, except with one extra point. We can compare them to check that the answers are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this to the original values. These are $10^{-14}$ times smaller. That's close enough to call it the same answer. The two codes implement the same calculation but use different methods, so some numerical error is expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "* Generate 1000 different noise datasets, using the same noise generation function as above.\n",
    "* For each dataset compute the cross-correlation with `signal_1`, `signal_2`, `signal_3` and `signal_4`. Store the largest value for each signal\n",
    "* (You should now have 4 length-1000 arrays with each value being the loudest value of the cross-correlation).\n",
    "* Plot a cumulative histogram of these values (Look up the matplotlib examples for examples of this).\n",
    "* Why are these histograms different for each signal?\n",
    "* How loud does a signal have to be to go from not-at-all significant to very significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
